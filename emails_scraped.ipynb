{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Collect emails from a website and store the results into a csv file<h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify website and download into 'soup' variable\n",
    "website = 'URL HERE' # place your link on the quotes\n",
    "source = requests.get(website).text\n",
    "\n",
    "soup = bs(source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will print out the html text in a readable way\n",
    "# it helps to ensure that the information was downloaded correctly\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file to store the emails\n",
    "csv_file = open('emails_scraped.csv', 'a') # the 'a' appends to an existing csv. A 'w', for example, \n",
    "                                           # would write over the existing content\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['email'])\n",
    "\n",
    "# loop through all the emails and store them onto csv\n",
    "for link in soup.find_all('a', class_='email'):\n",
    "    email = link.text                      # this extracts the text from the html code\n",
    "    print(email)                           # this just shows me what the code is doing \n",
    "    csv_writer.writerow([email])           # the code writes the email in a new excel row\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify website and download into 'soup' variable\n",
    "website = 'URL HERE'\n",
    "source = requests.get(website).text\n",
    "soup = bs(source, 'lxml')\n",
    "\n",
    "# create a csv file to store the emails\n",
    "csv_file = open('emails_scraped.csv', 'a')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# loop through all the emails and store them onto csv\n",
    "for link in soup.find_all('a', class_='btn btn-main btn-sm email'):\n",
    "    link = link.get('href')             # notice this doesn't use .text()!\n",
    "    email = link[7:]                    # notice that I'm slicing the first 7 characters off\n",
    "    print(email)\n",
    "    csv_writer.writerow([email])\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This website has some null values in it, which stops the loop I used in the previous examples. Instead, I included some conditional statements in order to skip these values and continue on to the next email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify website and download into 'soup' variable\n",
    "website = 'URL HERE'\n",
    "source = requests.get(website).text\n",
    "soup = bs(source, 'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file to store the emails\n",
    "csv_file = open('emails_scraped.csv', 'a')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# loop through all the emails and store them onto csv\n",
    "for link in soup.find_all('div', class_='tertiary'):\n",
    "    try:\n",
    "        link = link.find('a')        # some of the \"div\" tags have null values and cannot be extracted. This stops my code. Look below for how to fix!\n",
    "        email = link['href']         # notice how I used square brackets instead of .get()\n",
    "        email = email[7:]          \n",
    "        print(email)\n",
    "        csv_writer.writerow([email])\n",
    "    except: link = None             # this tells my code to not perform the loop if the value is a NoneType / null\n",
    "    \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4th Website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify website and download into 'soup' variable\n",
    "website = 'URL HERE'\n",
    "source = requests.get(website).text\n",
    "soup = bs(source, 'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file to store the emails\n",
    "csv_file = open('emails_scraped.csv', 'a')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# loop through all the emails and store them onto csv\n",
    "for link in soup.find_all('div', class_='details'):\n",
    "    try:\n",
    "        link = link.find('a')\n",
    "        link = link['href'] \n",
    "        email = link[7:]\n",
    "        print(email)\n",
    "        csv_writer.writerow([email])\n",
    "    except: link = None \n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5th website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This was the most challenging code. This website had multiple 'a' tags inside the parent 'div' tag, and only one of them (in each 'div') contained the email in the 'href' section. Thus, webscraping would only return the values in which the email was positions on the first 'a' tag, and this was the case for only SOME of the emails.\n",
    "\n",
    "#### So, instead of scraping the information directly as I did in all previous examples, I did the following:\n",
    "#### 1. I placed all the 'a' tags in a list, which would automatically save as tuples\n",
    "#### 2. I converted the tuples into lists elements so that I could manipulate the values\n",
    "#### 3. Converted each element into a string\n",
    "#### 4. Verified that the word \"mailto\" was somewhere in that element\n",
    "#### 5. If it was in the element, then it would slice out the first 36 characters and the last 76 characters and add it to the csv file\n",
    "#### 6. If not, then print \"no\". This helped me verify that the code is doing what I intend for it to do\n",
    "#### 7. Finally, the loop increased 'i by 1 so it could move on to the next element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify website and download into 'soup' variable\n",
    "website = 'YOUR URL'\n",
    "source = requests.get(website).text\n",
    "soup = bs(source, 'lxml')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a csv file to store the emails\n",
    "csv_file = open('emails_scraped.csv', 'a')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# initiate i\n",
    "i = 1\n",
    "\n",
    "# loop through all the 'a' tags and store them on a variable (it will automatically store as a touple)\n",
    "links = soup.find_all('a')\n",
    "# convert the touple into a list and enumerate each one\n",
    "link_list = [list(link) for link in enumerate(links)] # This is a one-line for loop. Here's a tutorial: https://www.youtube.com/watch?v=cGH8cVhj7H4 \n",
    "\n",
    "# loop through each element of the link_list\n",
    "for link in link_list:\n",
    "    try:\n",
    "        for link in link_list:\n",
    "            # set variables\n",
    "            current_link = str(link_list[i][1])        # convert item into string\n",
    "            email_isolated = current_link[36:-76]      # remove the first 36 and the last 76 characters \n",
    "            \n",
    "            \n",
    "            if 'mailto' in current_link:\n",
    "                csv_writer.writerow([email_isolated])\n",
    "                print(email_isolated)\n",
    "            else:\n",
    "                print('no')\n",
    "\n",
    "            # add 1 to i\n",
    "            i = i+1\n",
    "  #csv_writer.writerow([email])\n",
    "    except: link = None             \n",
    "    \n",
    "csv_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
